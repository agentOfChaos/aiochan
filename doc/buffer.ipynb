{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "# import os\n",
    "# os.environ['PYTHONASYNCIODEBUG'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you can try this tutorial in [![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/zh217/aiochan/master?filepath=doc%2Fbuffer.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channel buffering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Channels are used as meating points: a pair of put/get operations can only be completed when *both* involved parties are present at the same time. However, in practice, it is sometimes necessary to relax this requirement a little bit so that puts can complete immediately even when no one is there to get, and if previous puts are available, a get can complete without a put sitting there waiting. This behaviour where we further decouple put and get operations in time is called buffering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, buffering can be done without any further support from the library: we can have a pair of channels, `ch_in` and `ch_out`, acting as one, and a coroutine busy working in the background, promptly getting values from `ch_in` whenever they come in and store them onto some data structure, and at the same time feeding values to getters of `ch_out` whenever they come by. You can do this as an exercise (hint: use `select`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, to reduce clutter, and to improve performance, `Chan` has built-in support for buffer. In `aiochan`, buffering is always bounded: you have to decide at the onset how much pending stuff stored in your buffer you can tolerate. Some languages like Erlang nominally support unbounded buffering as the default, but the limit imposed by the operating system is always there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result a\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiochan as ac\n",
    "\n",
    "async def main():\n",
    "    c = ac.Chan(1)\n",
    "    \n",
    "    await c.put('a')\n",
    "    result = await c.get()\n",
    "    print('result', result)\n",
    "    \n",
    "ac.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, a buffered channel is created by having a positive number as the argument to the channel constructor. In this example, if there were no buffer, the example would deadlock: the first `await` would never complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positive number to the constructor signifies the size of the buffer. In the example, the size is one, so if we have two puts in a row the example would block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of *fixed length buffers*. The constructor call `Chan(1)` is actually a shorthand for `Chan('f', 1)`, `'f'` for fixed length. These buffers block on put when they are full. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixed length buffers are often used to implement back-pressure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "producing 1\n",
      "producing 2\n",
      "producing 3\n",
      "consuming 1\n",
      "producing 4\n",
      "producing 5\n",
      "consuming 2\n",
      "producing 6\n",
      "consuming 3\n",
      "producing 7\n",
      "consuming 4\n",
      "producing 8\n"
     ]
    }
   ],
   "source": [
    "async def worker(c):\n",
    "    i = 0\n",
    "    while True:\n",
    "        i += 1\n",
    "        await asyncio.sleep(0.05)\n",
    "        print('producing', i)\n",
    "        await c.put(i)\n",
    "\n",
    "async def consumer(c):\n",
    "    while True:\n",
    "        await asyncio.sleep(0.2)\n",
    "        result = await c.get()\n",
    "        print('consuming', result)\n",
    "\n",
    "async def main():\n",
    "    c = ac.Chan(3)\n",
    "    ac.go(worker(c))\n",
    "    ac.go(consumer(c))\n",
    "    await asyncio.sleep(1)\n",
    "    \n",
    "ac.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, producers and consumers are working at different rates. We want to ensure the consumer always have something to work with, so producers have to work ahead of consumers, but we also want to ensure that producers don't work so fast that the consumers can never catch up. A buffer solves the problem well. Our buffering solution still works even if the time taken to produce/consume items are somewhat random: within bounds, appropriate buffering can ensure minimal waiting while preventing producing and consuming rates from diverging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In situations that getters just can't keep up with putters *and* you definitely cannot tolerate blocking for producers (maybe because you don't control the producers), you have to make some compromise and use some other kinds of buffers which will *discard* some elements in exchange for non-blocking puts. We have built-in support for two of them: *dropping buffers* will just silent drop any more incoming puts when they become full:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    c = ac.Chan('d', 2) # 'd' for 'dropping'\n",
    "    await c.put(1)\n",
    "    await c.put(2)\n",
    "    await c.put(3)\n",
    "    c.close()\n",
    "    async for v in c:\n",
    "        print(v)\n",
    "\n",
    "ac.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look: the last value is missing. We also have *sliding buffers*, which when full, will drop the *earliest* pending value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    c = ac.Chan('s', 2) # 'd' for 'dropping'\n",
    "    await c.put(1)\n",
    "    await c.put(2)\n",
    "    await c.put(3)\n",
    "    c.close()\n",
    "    async for v in c:\n",
    "        print(v)\n",
    "\n",
    "ac.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beginning we have said that channels are used to circumvent the use of locks and semaphores so that our programs are easier to develop and easier to reason about. Well, sometimes locks and semaphores are the most natural solutions to a problem. And in such situations, *buffered channels can be used as locks and semaphores*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 is now working\n",
      "1 is now working\n",
      "2 is now working\n",
      "3 is now working\n",
      "4 is now working\n",
      "5 is now working\n",
      "6 is now working\n",
      "7 is now working\n",
      "8 is now working\n",
      "9 is now working\n"
     ]
    }
   ],
   "source": [
    "async def worker(lock, tag):\n",
    "    while True:\n",
    "        await lock.get()\n",
    "        print('%s is now working' % tag)\n",
    "        await asyncio.sleep(0.1)\n",
    "        await lock.put(True)\n",
    "\n",
    "async def main():\n",
    "    lock = ac.Chan(1).add(True)\n",
    "    for i in range(10):\n",
    "        ac.go(worker(lock, i))\n",
    "    await asyncio.sleep(1)\n",
    "    \n",
    "ac.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 1 second, only 10 operations complete even though we have 10 workers, whose maximum productivity is 100 operations in 1 second. In the presence of the lock, work becomes serial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a buffer size greater than 1 gives you a semaphore, which in our case increases the throughput:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 is now working\n",
      "1 is now working\n",
      "2 is now working\n",
      "3 is now working\n",
      "4 is now working\n",
      "5 is now working\n",
      "6 is now working\n",
      "7 is now working\n",
      "8 is now working\n",
      "9 is now working\n",
      "0 is now working\n",
      "1 is now working\n",
      "2 is now working\n",
      "3 is now working\n",
      "4 is now working\n",
      "5 is now working\n",
      "6 is now working\n",
      "7 is now working\n",
      "8 is now working\n",
      "9 is now working\n"
     ]
    }
   ],
   "source": [
    "async def worker(lock, tag):\n",
    "    while True:\n",
    "        await lock.get()\n",
    "        print('%s is now working' % tag)\n",
    "        await asyncio.sleep(0.1)\n",
    "        await lock.put(True)\n",
    "\n",
    "async def main():\n",
    "    lock = ac.Chan(2).add(True, True)\n",
    "    for i in range(10):\n",
    "        ac.go(worker(lock, i))\n",
    "    await asyncio.sleep(1)\n",
    "    \n",
    "ac.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But why would you want to use channels *as* locks when you can use the builtin locks from `asyncio`? Consistency and flexibility. Remember `select`? Now we can `select` on locks! You can do all kinds of funky stuff with `select` and locks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker-0 working\n",
      "worker-1 working\n",
      "worker-2 working\n",
      "worker-0 working\n",
      "worker-1 working\n",
      "worker-2 working\n",
      "worker-0 working\n",
      "worker-1 working\n",
      "worker-2 working\n",
      "worker-0 working\n",
      "worker-1 working\n",
      "worker-2 working\n",
      "worker-0 working\n",
      "worker-1 working\n",
      "worker-2 working\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "async def worker(locks, tag):\n",
    "    while True:\n",
    "        _, lock = await ac.select(*locks)\n",
    "        print('%s working' % tag)\n",
    "        await asyncio.sleep(0.1)\n",
    "        await lock.put(True)\n",
    "                \n",
    "async def main():\n",
    "    locks = [ac.Chan(1, name='lock%s' % i).add(True) for i in range(3)]\n",
    "    for i in range(3):\n",
    "        ac.go(worker(locks, 'worker-%s' % i))\n",
    "    await asyncio.sleep(0.5)\n",
    "    \n",
    "ac.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the worker proceeds whenever it can get its hand on *any* of a sequence of locks. With 3 locks we got 15 units of work done in half a second. You can change to 2 locks, in which case only 10 units of work would be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap:\n",
    "\n",
    "* Channels support buffering.\n",
    "* Fixed length buffering blocks on put when full, whereas dropping and sliding buffering never blocks but may throw away items when full.\n",
    "* Buffering can be used to implement back-pressure.\n",
    "* Buffered channels can be used as locks and semaphores, and you can `select` on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Congratulations!* Now you know *almost* everything you need to write non-trivial concurrency applications with `aiochan`. You are only limited by your imagination! Still, there are various *patterns* of concurrency programs that occur so often so that we have implemented them as additional functions and methods that you can readily use. None of them is essential, but using the provided *convenience* functions make your code easier to read and reason about."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
